{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from gensim.models import KeyedVectors\n",
    "from embedding_helpers import load_embeddings\n",
    "from debiasing import debias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(w,g):\n",
    "    \n",
    "    return np.dot(w,g)/(np.linalg.norm(w) * np.linalg.norm(g))\n",
    "    \n",
    "def DirectBias(evaluation_set, bias_direction, c=1):\n",
    "    \n",
    "    direct_bias = 0\n",
    "    \n",
    "    for w in evaluation_set:\n",
    "        direct_bias += abs(cos_similarity(w, bias_direction))**c\n",
    "        \n",
    "    return direct_bias/len(evaluation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n"
     ]
    }
   ],
   "source": [
    "# We will use the 400K sample since the other samples will kill the machine\n",
    "glove = load_embeddings('data/glove.6B.100d.txt')\n",
    "word2vec = KeyedVectors.load_word2vec_format('data/word2vec-google-news-300/word2vec-google-news-300.gz', limit=600000, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our two endpoints for our hispanic-american axis in both embeddings\n",
    "mexican_vector_glove = np.array(glove[glove.index == 'mexican'])[0]\n",
    "american_vector_glove = np.array(glove[glove.index == 'american'])[0]\n",
    "\n",
    "mexican_vector_word2vec = word2vec.get_vector('mexican')\n",
    "american_vector_word2vec = word2vec.get_vector('american')\n",
    "\n",
    "# Get directions in both embeddings\n",
    "direction_glove = american_vector_glove - mexican_vector_glove\n",
    "direction_word2vec = american_vector_word2vec - mexican_vector_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Professions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profession words to check\n",
    "professions_glove = ['businessman','manager','legislator','maid','waiter','waitress','janitor'\n",
    "                     ,'doorman','custodian','gardener','landscaper','stonemason'\n",
    "                     ,'governor','doctor','nurse','attorney','lawyer','dentist','astronaut'\n",
    "                     ,'plumber','barber','hairdresser','cashier','dishwasher','nanny','manicurist'\n",
    "                     ,'bartender','carpenter','programmer','ceo','vp','executive','accountant']\n",
    "\n",
    "# word2vec has capitalized letters, so list has to be slightly modified\n",
    "professions_word2vec = ['businessman','manager','legislator','maid','waiter','waitress','janitor'\n",
    "                        ,'doorman','custodian','gardener','landscaper','stonemason'\n",
    "                        ,'governor','doctor','nurse','attorney','lawyer','dentist','astronaut'\n",
    "                        ,'plumber','barber','hairdresser','cashier','dishwasher','nanny','manicurist'\n",
    "                        ,'bartender','carpenter','programmer','CEO','VP','executive','accountant']\n",
    "\n",
    "profession_vectors_glove = list()\n",
    "for profession in professions_glove:\n",
    "    profession_vectors_glove.append(glove[glove.index == profession].to_numpy()[0])\n",
    "    \n",
    "profession_vectors_word2vec = list()\n",
    "for profession in professions_word2vec:\n",
    "    profession_vectors_word2vec.append(word2vec.get_vector(profession))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professions\n",
      "\n",
      "Direct Bias (Glove):  0.12\n",
      "Direct Bias (word2vec):  0.08\n"
     ]
    }
   ],
   "source": [
    "bias_professions_glove = DirectBias(profession_vectors_glove, direction_glove)\n",
    "bias_professions_word2vec = DirectBias(profession_vectors_word2vec, direction_word2vec)\n",
    "\n",
    "print(\"Professions\\n\")\n",
    "print(\"Direct Bias (Glove): \", round(bias_professions_glove, 2))\n",
    "print(\"Direct Bias (word2vec): \", round(bias_professions_word2vec, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "antonyms = ['best','worst'\n",
    "            ,'tasty','nasty'\n",
    "            ,'clean','dirty'\n",
    "            ,'employed','unemployed'\n",
    "            ,'beautiful','ugly'\n",
    "            ,'safe','dangerous'\n",
    "            ,'capable','incapable'\n",
    "            ,'early','late'\n",
    "            ,'succeed','fail'\n",
    "            ,'gentle','rough'\n",
    "            ,'brave','cowardly'\n",
    "            ,'intelligent','stupid'\n",
    "            ,'superior','inferior'\n",
    "            ,'diligent','lazy'\n",
    "            ,'quiet','noisy'\n",
    "            ,'pleasant','unpleasant'\n",
    "            ,'pure','impure'\n",
    "            ,'qualified','unqualified'\n",
    "            ,'courteous','rude'\n",
    "            ,'sober','drunk'\n",
    "            ,'safe','unsafe'\n",
    "            ,'useful','useless'\n",
    "            ,'obedient','disobedient'\n",
    "            ,'neat','messy']\n",
    "\n",
    "antonym_vectors_glove = list()\n",
    "for antonym in antonyms:\n",
    "    antonym_vectors_glove.append(glove[glove.index == antonym].to_numpy()[0])\n",
    "    \n",
    "antonym_vectors_word2vec = list()\n",
    "for antonym in antonyms:\n",
    "    antonym_vectors_word2vec.append(word2vec.get_vector(antonym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antonyms\n",
      "\n",
      "Direct Bias (Glove):  0.09\n",
      "Direct Bias (word2vec):  0.05\n"
     ]
    }
   ],
   "source": [
    "bias_antonyms_glove = DirectBias(antonym_vectors_glove, direction_glove)\n",
    "bias_antonyms_word2vec = DirectBias(antonym_vectors_word2vec, direction_word2vec)\n",
    "\n",
    "print(\"Antonyms\\n\")\n",
    "print(\"Direct Bias (Glove): \", round(bias_antonyms_glove, 2))\n",
    "print(\"Direct Bias (word2vec): \", round(bias_antonyms_word2vec, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutralizing\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n"
     ]
    }
   ],
   "source": [
    "glove_debiased = debias(glove\n",
    "                        , np.array(glove[glove.index == 'american'])[0] - np.array(glove[glove.index == 'mexican'])[0]\n",
    "                        , [('american','mexican')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mexican_vector_glove_debiased = np.array(glove_debiased[glove_debiased.index == 'mexican'])[0]\n",
    "american_vector_glove_debiased = np.array(glove_debiased[glove_debiased.index == 'american'])[0]\n",
    "\n",
    "# Get directions in both embeddings\n",
    "direction_glove_debiased = american_vector_glove_debiased - mexican_vector_glove_debiased\n",
    "\n",
    "\n",
    "profession_vectors_glove_debiased = list()\n",
    "for profession in professions_glove:\n",
    "    profession_vectors_glove_debiased.append(glove_debiased[glove_debiased.index == profession].to_numpy()[0])\n",
    "\n",
    "bias_professions_glove_debiased = DirectBias(profession_vectors_glove_debiased, direction_glove_debiased)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
